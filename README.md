# 搜索引擎项目一：文本预处理

## 概述

这是2020年SWJTU搜索引擎课程的第一个项目，项目的主要功能是**爬取足够数量的网页并对其进行预处理**，以便后续对其进行索引。

我的代码基于Python实现，代码里使用了这些第三方模块：

- bs4
- hanlp

另外还引用了位于[这里](https://tartarus.org/martin/PorterStemmer/python.txt)的Porter Stemmer算法Python实现。

## 实现过程

### 网页下载器/爬虫

这部分的代码实现位于`crawler.py`文件中，对国内知名媒体**IT之家**和英文**维基百科**两个站点上的文档自动进行批量下载。

#### save_html()

下载并保存单个网页是通过`save_html()`函数实现的，这个函数提供了两种不同的下载方法：使用使用`get_html()`下载函数（下面详细描述），或者系统自带模块`urllib.request`。使用时向`save_html()`函数传递网页的URL、保存的文件名和类别，然后函数就会自动按类别创建文件夹，然后将URL指向的网页按指定的文件名保存到这个文件夹里。

#### get_html()

`get_html()`函数是一个使用套接字编程实现的网页下载函数。传入网页URL后，首先判断URL格式是否合法，如果合法就接着判断URL采用的是`HTTP`协议还是`HTTPS`协议，然后对应地，分别创建普通套接字和带SSL包装的套接字，最后调用`get_response_body()`获取网页内容，如果成功获取到网页内容就以字符串形式返回网页的内容，否则抛出错误。

#### get_response_body()

这是一个用于获取HTTP响应正文的函数。设计上，该函数遵从`HTTP1.1`协议（不支持`HTTP2.0/HTTP3.0`），当接收到URL参数后，函数构造一个最小的`HTTP1.1 GET`请求报文，请求头部仅包含`Host`信息，例如：

``` HTTP
GET / HTTP/1.1
Host: www.baidu.com
```

然后，通过参数中传入的套接字连接到目标主机，发送HTTP请求报文，并等待目标主机的回复。正常情况下，目标主机通过套接字返回的HTTP响应报文包括状态行、响应头、响应正文三部分，一个充分实现了HTTP协议的解析程序需要能够识别这三部分内包括的所有信息。由于本函数的目的只是下载**响应正文**中的数据，所以并没有充分实现HTTP协议，而是做了大幅简化。

要下载响应正文，首先需要知道响应正文的长度，这一数据包括在响应头的`Content-length`字段中，于是思路便是：从套接字中读取数据，直到响应头完全被接收，然后解析得到响应头中的`Content-length`字段值，读取对应长度的响应正文数据。根据协议格式，响应头和响应正文间用一个空行`\r\n`隔开，加上响应头最后的一个`\r\n`，当套接字中接收到连续的`\r\n\r\n`四个字符就可以判定响应头（包括状态行）已经完全被接收，至此为第一步。第二步使用正则表达式匹配状态行信息，获取HTTP响应状态，如果遇到非正常情况（状态码不是200），则直接抛出错误，否则继续。第三步再次使用正则表达式匹配`Content-length`字段，就可以获取到响应正文的长度。

获取到正文长度信息后，只需要循环读取套接字中收到的数据，直至正文接收完毕，就可以以字符串形式返回正文数据。

由于本函数只实现了对`HTTP1.1`的支持，可能无法下载部分站点上的文档，所以`save_html()`函数中还提供了基于`urllib.request`系统模块的下载选项。

#### 爬取网页并保存

实现了网页下载功能后，爬虫就比较简单了。对目标站点的URL格式进行研究可以发现，IT之家的文章URL格式可以用`https://www.ithome.com/0/<i>/<j>.htm`描述，其中`j`的取值范围是`100-999`，溢出后向`i`进一，2020年4月15日`i`已经增长至`482`。这里编写程序读取`481-482`这一范围内的`i`对应的文章，共爬取到`1216`篇有效文档。维基百科的文章URL格式可以用`https://en.wikipedia.org/wiki/<article_name>`描述，由于文章名称没有规律，所以需要预先获取一份文章名称列表。这里使用了维基百科用户West.andrew.g提供的一份最流行的5000个维基百科页面的[列表](https://en.wikipedia.org/wiki/User:West.andrew.g/Popular_pages)，使用`JavaScrip`简单处理以后将其保存为txt文件以供程序自动爬取，共下载了`602`篇有效文档。

### 预处理

预处理模块对英文文档进行了如下操作：

- 提取正文内容
- 将各个单词进行字符化，完成删除特殊字符、大小写转换等操作
- 删除英文停用词（Stop Word）
- 调用或者编程实现英文Porter Stemming功能
- 经过以上处理之后，将经过处理之后所形成简化文档保存（如：News_1_E.txt），以备以后的索引处理

对中文文档进行了如下操作：

- 提取正文内容
- 调研并选择合适的中文分词技术和工具实现中文分词
- 删除中文停用词
- 将中文文档进行字符化，即可被搜索引擎索引的字符单元
- 经过以上处理之后，将经过处理之后所形成简化文档保存（如：News_1_C.txt），以备以后的索引处理

这部分的代码实现位于`preprocessor.py`文件中

#### 对英文文档的处理

对于每一个HTML文档，我们首先将其所有内容读入程序，这里使用Python的with语法结构，可以保证文件的正确关闭：

```Python
with open(file_path, "r", encoding="UTF-8") as f:
    html_content = f.read()
    ...
```

##### 提取正文内容

网页的源代码中存在大量HTML标签、CSS代码、JavaScript代码和标注信息，这些内容都会干扰搜索引擎的索引，所以要在预处理过程中去除。去除后，这里只保留文章正文的内容。由于不同站点页面的格式不同，提取正文的程序需要针对特定站点编写。

提取网页正文的第一步是解析网页的HTML源文件，这一工作通过`Beautiful Soup`模块实现。`Beautiful Soup`是一个功能强大的模块，主要功能是从网页抓取数据，它提供了一些简单的函数来解析HTML文档，并为用户提供需要抓取的数据。这里首先利用之前读取的文档内容构造一个`BeautifulSoup`类的实例，以备后续处理。

```Python
    parsed_content = BeautifulSoup(html_content, 'html.parser')
```

通过查阅浏览器的开发者模式发现，一篇维基百科文档的正文部分都位于一个`id`属性为`mw-content-text`的`div`标签内部，正文段落和标题分别用`p`和`h2`等标签包裹。这里直接用`Beautiful Soup`提供的`find()`函数找到正文所在的`div`标签，遍历其内部所有标签，一旦发现正文段落或标题就将其内容追加到`text_content`后。经过验证，这样可以准确地提取正文内容。

```Python
    text_content = ""
    # Extract pure-text content from the original html file
    for child in parsed_content.find(id="mw-content-text").div.children:
        if child.name in ("p", "h2", "h3", "h4", "h5"):
            text_content += child.get_text()
```

##### 字符化、删除特殊字符、大小写转换

#### 对中文文档的处理
